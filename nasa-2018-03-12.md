Scaling Scientific Python
-------------------------

<img src="images/dask_horizontal_white.svg" width="30%">
<img src="images/xarray.png" width=30%>
<img src="images/jupyterhub.svg" width=30%>

*Matthew Rocklin*

Anaconda Inc.


What to expect
--------------

1.  **Scaling NumPy**
    -  Complex datasets - *solved by XArray*
    -  Large datasets - *solved by Dask*
2.  **Distributed Systems**
    -  Dask networks
    -  HPC Deployments
    -  Cloud Deployments [pangeo.pydata.org](http://pangeo.pydata.org)
    -  JupyterHub + Dask
3.  **Lessons Learned**
    -  Data storage
    -  User environments
    -  Collaborators and people


### Problem: Our ability to produce datasets

<hr>

### Has outstripped our ability to analyze them


### The Numeric Python Ecosystem

-  NumPy, Pandas, Scikit-Learn, Matplotlib, ...

<hr>

### Serves Scientific Communities

-  Effiicient compiled code
-  Intuitive interfaces designed for non-experts
-  Thousands of libraries that work well together

<hr>

### ... but these don't scale out well


### NumPy is fast and intuitive

```python
>>> import numpy as np
>>> x = np.random.random((1000, 1000))      # Array of random data

>>> x[0, :5]                                # Access a few elements
array([0.90896491, 0.32975274, 0.19095781, 0.67890296, 0.33155882])
Wall time: 25.5 µs

>>> x.sum()                                 # Compute aggregations
500199.7715862236
Wall time: 633 µs

>>> (x + x.T - x.mean(axis=0)).std()        # Perform more complex operations
0.4083831772871384
Wall time: 30.2 ms
```


### Problem: Complex datasets break intuition

Real-world datasets contain several arrays with complex relationships.

```python
>>> import netCDF4
>>> data = netCDF4.Dataset('myfile.nc')
>>> list(data.variables)
['temperature',
 'pressure',
 'wind-velocity-x',
 'wind-velocity-y',
 'elevation',
 'latitude',
 'longitude',
 'time',
 'ensemble-id']

>>> data['temperature'][:] = data['temperature'][:, 1000:5000, 2000:6000]
>>> data['pressure'][:] = data['pressure'][:, 1000:5000, 2000:6000]
>>> ...
```

This burdens scientific users:

-  Track coordinates manually
-  Consistently transform many variables
-  Consistently operate across many files


### XArray: netCDF meets Pandas DataFrames

<img src="images/xarray-boxes-2.png" alt="XArray" width="100%">

```python
>>> ds = xarray.open_dataset('myfile.nc')

# xarray style
>>> ds.sel(time='2018-01-08').max(dim='ensemble')

# numpy Style
>>> array[[0, 1, 2, 3], :, :].max(axis=2)
```

*Taken from Stephan Hoyer's [ECMWF talk](https://docs.google.com/presentation/d/16CMY3g_OYr6fQplUZIDqVtG-SKZqsG8Ckwoj2oOqepU/edit#slide=id.g2b68f9254d_1_27)*


### XArray: makes scalable data analysis easy

```python
import xarray
ds = xarray.open_mfdataset('all/your/data/*.nc')            # Open many files
climatology = ds.groupby('time.season').mean('time')        # Compute seasonal average
temperature_range = abs(climatology.air.sel(season='JJA')   # Compare seasons
                      - climatology.air.sel(season='DJF'))
temperature_range.plot()                                    # Visualize
```

<hr>

### Feels native for geoscience community

### But general purpose, and integrates with wider ecosystem


### Problem: Scale

Scientific Python was not designed to scale

```python
>>> import numpy as np
>>> x = np.random.random((1000, 1000))
.
```

-  Could work from disk, loading and processing data chunk-by-chunk
-  Could use distributed memory on clusters of computers


### Problem: Scale

Scientific Python was not designed to scale

```python
>>> import numpy as np
>>> x = np.random.random((100000, 100000))
MemoryError
```

-  Could work from disk, loading and processing data chunk-by-chunk
-  Could use distributed memory on clusters of computers


### Dask.array scales Numpy

<img src="images/dask-array.svg" width="60%">

```python
import numpy as np                          # NumPy Arrays
x = np.random.random((1000, 1000))
y = (x + x.T - x.mean(axis=0)).std()

import dask.array as da                     # Dask Arrays
x = da.random.random((10000, 10000), chunks=(1000, 1000)))
y = (x + x.T - x.mean(axis=0)).std().compute()
```


<img src="http://dask.pydata.org/en/latest/_images/dask_horizontal_white.svg"
     alt="dask logo"
     width="40%">

<img src="images/grid_search_schedule.gif" width="100%">

-  Parallel task scheduler for Python
-  Parallelizes Numpy, Pandas, SKLearn, ...
-  Developed by NumPy, Pandas, SKLearn, Jupyter communities
-  Light weight, well supported, BSD licensed


### Dask also parallelizes ...

<img src="images/credit_models/simple-model.svg" width=50% align="right">

-  Pandas
-  GeoPandas
-  Scikit-Image
-  Scikit-Learn
-  Concurrent.futures
-  ...
-  Several private systems



### Distributed Systems


### Dask network

<img src="images/network-inverse.svg">

1.  **Scheduler**: coordinates everyone, does no actual work
2.  **Workers**: do work, store data, communicate peer-to-peer
3.  **Clients**: ask for work, share pointers to datasets


### Can set things up by hand

```bash
user@host $ dask-scheduler
Scheduler running on 192.168.0.1:8786

user@host1$ dask-worker 192.168.0.1:8786
user@host2$ dask-worker 192.168.0.1:8786
user@host3$ dask-worker 192.168.0.1:8786
```

<hr>

```python
>>> from dask.distributed import Client
>>> client = Client('192.168.0.1:8786')
```

### But we usually do this with a resource manager


### Often used with the following:

1.  HPC job schedulers
    -  PBS
    -  Grid Engine
    -  SLURM
    -  LSF
    -  Torque, condor, DRMAA-compliant system ...
2.  Ad-hoc clusters
    -  SSH
    -  Home grown job schedulers
3.  Loosely coupled job schedulers
    -  Yarn (Hadoop's default system)
    -  Mesos
    -  **Kubernetes**

[dask.pydata.org/en/latest/setup.html](http://dask.pydata.org/en/latest/setup.html)


### This is easy for users

    $ pip install dask-jobqueue

```python
>>> from dask_joqqueue import PBSCluster
>>> cluster = PBSCluster(n_workers=20, project='my-project', ...)

>>> from dask.distributed import Client
>>> client = Client(cluster)
```


### This is easy for users

    $ pip install dask-kubernetes

```python
>>> from dask_kubernetes import KubeCluster
>>> cluster = KubeCluster(n_workers=20, namespace='my-namespace')

>>> from dask.distributed import Client
>>> client = Client(cluster)
```


### Adapt cluster size based on workload

    $ pip install dask-kubernetes

```python
>>> from dask_kubernetes import KubeCluster
>>> cluster = KubeCluster(namespace='my-namespace')
>>> cluster.adapt(minimum=5, maximum=100)  # dynamically scale between 5-100 workers

>>> from dask.distributed import Client
>>> client = Client(cluster)
```



### [pangeo.pydata.org](http://pangeo.pydata.org)

### Lets do all of this on the cloud


### Infrastructure

-  Deployment
    -  Kubernetes, on Google, but would work anywhere

       Helm for configuration

       Docker and Conda to specify user environments
    -  JupyterLab for user environments
    -  JupyterHub for user management
    -  Dask workers launched from user notebooks
    -  Use auto-scaling node pools to manage costs
-  Storage:
    -  HDF files + FUSE on cloud object store (like S3)
    -  Zarr on cloud object store directly


### JupyterHub and Dask on Kubernetes

User connects to JupyterHub

<img src="images/pangeo/jupyter-hub.1.svg">


### JupyterHub and Dask on Kubernetes

JupyterHub launches notebook environment for User

<img src="images/pangeo/jupyter-hub.2.svg">


### JupyterHub and Dask on Kubernetes

User launches Dask from cloud environment

<img src="images/pangeo/jupyter-hub.3.svg">


### JupyterHub and Dask on Kubernetes

Second user arrives and connects to JupyterHub

<img src="images/pangeo/jupyter-hub.4.svg">


### JupyterHub and Dask on Kubernetes

JupyterHub launches second isolated cloud environment

<img src="images/pangeo/jupyter-hub.5.svg">


### JupyterHub and Dask on Kubernetes

Cluster scales out to accomodate new user

<img src="images/pangeo/jupyter-hub.6.svg">


### JupyterHub and Dask on Kubernetes

New user also launches Dask

<img src="images/pangeo/jupyter-hub.7.svg">


### JupyterHub and Dask on Kubernetes

First user goes home for the day

<img src="images/pangeo/jupyter-hub.8.svg">


### JupyterHub and Dask on Kubernetes

User's cloud environment retires

<img src="images/pangeo/jupyter-hub.9.svg">


### JupyterHub and Dask on Kubernetes

Cluster scales down to reduce costs

<img src="images/pangeo/jupyter-hub.10.svg">


### Operationally we see...

1.  A few users on at a time
2.  One or two operating at scale

    ```
    $ kubectl get pods -n pangeo
    NAME                              READY     STATUS    RESTARTS   AGE
    jupyter-hendra-2dherviawan        1/1       Running   0          14m
    jupyter-philippjfr                1/1       Running   0          38m
    jupyter-rsignell-2dusgs           1/1       Running   0          55m
    jupyter-vincentschut              1/1       Running   0          5h
    jupyter-willirath                 1/1       Running   0          36m

    dask-philippjfr-403975b1-a4pg5p   1/1       Running   0          15m
    dask-philippjfr-403975b1-a6hhvl   1/1       Running   0          15m
    dask-philippjfr-403975b1-a882kj   1/1       Running   0          15m
    ...
    dask-philippjfr-403975b1-aztvqg   1/1       Running   0          15m
    dask-philippjfr-403975b1-a882kj   1/1       Running   0          15m

    hub-68b7fb696-jdfc8               1/1       Running   0          40m
    proxy-584cf885d7-xbns5            2/2       Running   0          40m
    ```


### Operationally we see...

Costs about $1 / hour with preemptible nodes

<img src="images/pangeo/gcp-autoscaling.png" width="50%">


### Others have done this too

-   UK Met Office's JADE
-   USGS + HDF group collaboration on AWS
-   University of Washington on AWS
-   Someone is setting it up on Jetstream?


### You don't have to do it this way

### Life is simpler on traditional data centers

1.  You already have user management
2.  You already have billing/quotas
3.  You already have data access
4.  You already have local software environments
5.  You already have paid staff to troubleshoot



### Opportunities and Challenges


### Opportunity 1: Broad collaboration

-  **Yu Cheng**: Pysical oceanography in Miami, PhD student
-  **J Gerard**: Data engineer at Rhodium Group, Climate impact lab
-  **Nic Wayand**: University of Saskatchewan, Cold water lab, Postdoc
-  **Vincent Schut**: Satelligence, Netherlands
-  **Gregory R Lee**: Cincinnati Children's Hospital, Neuro-imaging
-  **Guillaume EB**: Distributed systems engineer, CNES
-  **M Hendra Herviawan**: Bandung, Indonesia.  Marketing and data analysis
-  **Marius van Niekerk**: Engineer at Flatiron Health

<hr>

*People who have logged in to pangeo.pydata.org in the last 48 hours about whom
I could find some information, not including collaborators*


### Challenge 1: Custom environments

*Everyone wants a different software environment*

-  Small-scale customizability
    -  JupyterLab: Users have a normal Linux environment
    -  JupyterHub: Users have a persistent cloud drive

-  Large-scale customizability options:
    -  Several independent deployments
    -  Curated set of Docker images per deployment
    -  Accept user-defined environments (like mybinder.org)


### Opportunity 2: Shared Data Storage

-   Organizations like NASA, NCAR host large datasets

    Both on cloud storage and traditional data centers

-   Increasing access enables new groups to do science cheaply

    -  Students
    -  Smaller institutions
    -  Automated services

-   Python ecosystem has tools to read most data formats

    Once we give it access to bytes


### Challenge 2: Cloud-friendly data formats

-  The HDF library expects a POSIX file handle
    -  Mimic file handle with an object store (FUSE)
    -  Teach HDF how to read from S3, GCS, ...
    -  ...

-  Invent new formats and standards (like Zarr)

-  Give our data to a company, let them handle it
    -  HDF group has a prototype solution
    -  Cloud vendors may want to do this

-  Not as much a problem for image formats, like GeoTIFF


### Custom user environments and cloud data storage

### are the two largest barriers to cloud adoption

